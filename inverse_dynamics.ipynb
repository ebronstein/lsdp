{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import functools\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from diffusion_policy.common.normalize_util import get_image_range_normalizer\n",
    "from diffusion_policy.common.pytorch_util import compute_conv_output_shape\n",
    "from diffusion_policy.common.sampler import get_val_mask\n",
    "from diffusion_policy.dataset.pusht_image_dataset import PushTImageDataset\n",
    "from diffusion_policy.model.common.normalizer import (\n",
    "    LinearNormalizer,\n",
    "    SingleFieldLinearNormalizer,\n",
    ")\n",
    "from vae.pusht_vae import VanillaVAE"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "path = \"/nas/ucb/ebronstein/lsdp/diffusion_policy/data/pusht/pusht_cchi_v7_replay.zarr\"\n",
    "dataset = PushTImageDataset(path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "episode = dataset.replay_buffer.get_episode(0)\n",
    "for key, value in episode.items():\n",
    "    print(key, value.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "# Visualize subset of the episode\n",
    "episode_len = episode[\"img\"].shape[0]\n",
    "time_steps = np.linspace(0, episode_len - 1, 25).astype(int)\n",
    "# Plot the first few images and actions in the episode\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "for i, step in enumerate(time_steps):\n",
    "    ax = axs[i // 5, i % 5]\n",
    "    ax.imshow(episode[\"img\"][step] / 255.0)\n",
    "    ax.set_title(episode[\"action\"][step])\n",
    "    ax.axis(\"off\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "episode[\"state\"].min(axis=0), episode[\"state\"].max(axis=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episode data sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "episode = dataset.replay_buffer.get_episode(0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "states = episode[\"state\"]\n",
    "actions = episode[\"action\"]\n",
    "\n",
    "norm_states = normalize_pn1(states, min_state, max_state)\n",
    "norm_actions = normalize_pn1(actions, min_action, max_action)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "offset = 2\n",
    "plt.plot(states[offset:, :2], label=\"state\")\n",
    "plt.plot(actions[:-offset], label=\"action\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "for offset in range(1, 21):\n",
    "    diff = states[offset:, :2] - actions[:-offset]\n",
    "    norm_diff = norm_states[offset:, :2] - norm_actions[:-offset]\n",
    "\n",
    "    mse = (diff**2).sum(axis=-1).mean()\n",
    "    me = np.linalg.norm(diff, axis=-1).mean()\n",
    "\n",
    "    norm_mse = (norm_diff**2).mean()\n",
    "    norm_me = np.linalg.norm(norm_diff, axis=-1).mean()\n",
    "    print(\n",
    "        f\"Offset {offset}: MSE {mse}, normalized MSE {norm_mse}, ME {me}, normalized ME {norm_me}\"\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "# Load the VAE\n",
    "img_data = torch.from_numpy(dataset.replay_buffer[\"img\"]).permute(0, 3, 1, 2).float()\n",
    "N, C, H, W = img_data.shape\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vae = VanillaVAE(in_channels=C, in_height=H, in_width=W, latent_dim=32).to(device)\n",
    "save_dir = \"models/pusht_vae\"\n",
    "vae.load_state_dict(torch.load(os.path.join(save_dir, \"vae_32_20240403.pt\")))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode dataset with VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "def encode_images(img, img_normalizer, vae, device):\n",
    "    img = img_normalizer(img / 255.0)\n",
    "    with torch.no_grad():\n",
    "        mu, log_var = vae.encode(img.to(device))\n",
    "    return mu.cpu().detach().numpy()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "# Encode the full dataset by batches to avoid CUDA OOM.\n",
    "img_normalizer = get_image_range_normalizer()\n",
    "batch_size = 128\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mu_list = []\n",
    "vae.eval()\n",
    "for i in trange(0, N, batch_size):\n",
    "    img_batch = img_data[i : i + batch_size]\n",
    "    batch_mu = encode_images(img_batch, img_normalizer, vae, device)\n",
    "    mu_list.append(batch_mu)\n",
    "\n",
    "encoded_imgs = np.concatenate(mu_list, axis=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "source": [
    "# Make the latent image normalizer\n",
    "encoded_imgs_mean = encoded_imgs.mean(axis=0)\n",
    "encoded_imgs_std = encoded_imgs.std(axis=0)\n",
    "\n",
    "encoded_imgs_scale = (1.0 / encoded_imgs_std).astype(np.float32)\n",
    "encoded_imgs_offset = -encoded_imgs_mean / encoded_imgs_std\n",
    "encoded_imgs_stat = {\n",
    "    \"mean\": encoded_imgs_mean,\n",
    "    \"std\": encoded_imgs_std,\n",
    "    \"min\": encoded_imgs.min(axis=0),\n",
    "    \"max\": encoded_imgs.max(axis=0),\n",
    "}\n",
    "latent_img_normalizer = SingleFieldLinearNormalizer.create_manual(\n",
    "    scale=encoded_imgs_scale,\n",
    "    offset=encoded_imgs_offset,\n",
    "    input_stats_dict=encoded_imgs_stat,\n",
    ")\n",
    "\n",
    "# Make the action normalizer.\n",
    "max_action = dataset.replay_buffer[\"action\"].max(axis=0)\n",
    "min_action = np.zeros_like(max_action)\n",
    "\n",
    "# Make the state normalizer.\n",
    "max_state = dataset.replay_buffer[\"state\"].max(axis=0)\n",
    "min_state = np.zeros_like(max_state)\n",
    "\n",
    "\n",
    "def normalize_pn1(x, min_val, max_val):\n",
    "    # Normalize to [0, 1]\n",
    "    nx = (x - min_val) / (max_val - min_val)\n",
    "    # Normalize to [-1, 1]\n",
    "    return nx * 2 - 1\n",
    "\n",
    "\n",
    "def denormalize_pn1(nx, min_val, max_val):\n",
    "    # Denormalize from [-1, 1]\n",
    "    x = (nx + 1) / 2\n",
    "    # Denormalize from [0, 1]\n",
    "    return x * (max_val - min_val) + min_val"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episode Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class EpisodeDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset,\n",
    "        n_obs_history=1,\n",
    "        n_pred_horizon=1,\n",
    "        episode_idxs=None,\n",
    "        process_img_fn=None,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with the main dataset object that contains\n",
    "        the replay_buffer. Also, specify the lengths of observation history\n",
    "        and prediction horizon.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.n_obs_history = n_obs_history\n",
    "        self.n_pred_horizon = n_pred_horizon\n",
    "        self.episode_idxs = list(episode_idxs)\n",
    "        self.process_img_fn = process_img_fn\n",
    "        self.device = device\n",
    "        self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the episodes to create a flat list of samples.\n",
    "        Each sample is a tuple of dictionaries: (obs_history, pred_horizon).\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "\n",
    "        if self.episode_idxs is None:\n",
    "            self.episode_idxs = range(self.dataset.replay_buffer.n_episodes)\n",
    "\n",
    "        for episode_idx in tqdm(self.episode_idxs, desc=\"Preparing data\"):\n",
    "            episode = self.dataset.replay_buffer.get_episode(episode_idx)\n",
    "            img = episode[\"img\"].transpose(0, 3, 1, 2)  # CHW format\n",
    "            if self.process_img_fn is not None:\n",
    "                img = self.process_img_fn(img)\n",
    "            actions = torch.tensor(episode[\"action\"], dtype=torch.float32).to(\n",
    "                self.device\n",
    "            )\n",
    "            states = torch.tensor(episode[\"state\"], dtype=torch.float32).to(self.device)\n",
    "\n",
    "            # Iterate through the episode to create samples with observation history and prediction horizon\n",
    "            for i in range(len(actions) - self.n_obs_history - self.n_pred_horizon + 1):\n",
    "                obs_history_imgs = img[i : i + self.n_obs_history]\n",
    "                obs_history_actions = actions[i : i + self.n_obs_history]\n",
    "                obs_history_states = states[i : i + self.n_obs_history]\n",
    "\n",
    "                pred_horizon_imgs = img[\n",
    "                    i\n",
    "                    + self.n_obs_history : i\n",
    "                    + self.n_obs_history\n",
    "                    + self.n_pred_horizon\n",
    "                ]\n",
    "                pred_horizon_actions = actions[\n",
    "                    i\n",
    "                    + self.n_obs_history : i\n",
    "                    + self.n_obs_history\n",
    "                    + self.n_pred_horizon\n",
    "                ]\n",
    "                pred_horizon_states = states[\n",
    "                    i\n",
    "                    + self.n_obs_history : i\n",
    "                    + self.n_obs_history\n",
    "                    + self.n_pred_horizon\n",
    "                ]\n",
    "\n",
    "                obs_history = {\n",
    "                    \"img\": obs_history_imgs,\n",
    "                    \"action\": obs_history_actions,\n",
    "                    \"state\": obs_history_states,\n",
    "                }\n",
    "                pred_horizon = {\n",
    "                    \"img\": pred_horizon_imgs,\n",
    "                    \"action\": pred_horizon_actions,\n",
    "                    \"state\": pred_horizon_states,\n",
    "                }\n",
    "\n",
    "                self.samples.append((obs_history, pred_horizon))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples across all episodes.\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return the idx-th sample from the dataset.\n",
    "        \"\"\"\n",
    "        obs_history, pred_horizon = self.samples[idx]\n",
    "\n",
    "        # Convert data to PyTorch tensors and ensure the data type is correct\n",
    "        # for key, value in obs_history.items():\n",
    "        #     obs_history[key] = torch.tensor(value, dtype=torch.float32)\n",
    "        # for key, value in pred_horizon.items():\n",
    "        #     pred_horizon[key] = torch.tensor(value, dtype=torch.float32)\n",
    "\n",
    "        return obs_history, pred_horizon"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "source": [
    "# Make train and val loaders\n",
    "val_mask = get_val_mask(dataset.replay_buffer.n_episodes, 0.1)\n",
    "val_idxs = np.where(val_mask)[0]\n",
    "train_idxs = np.where(~val_mask)[0]\n",
    "\n",
    "# Make the episode dataset and create a DataLoader.\n",
    "batch_size = 256\n",
    "n_obs_history = 5\n",
    "n_pred_horizon = 0\n",
    "process_img_fn = functools.partial(\n",
    "    encode_images, img_normalizer=img_normalizer, vae=vae, device=device\n",
    ")\n",
    "train_episode_dataset = EpisodeDataset(\n",
    "    dataset,\n",
    "    n_obs_history=n_obs_history,\n",
    "    n_pred_horizon=n_pred_horizon,\n",
    "    episode_idxs=train_idxs,\n",
    "    process_img_fn=process_img_fn,\n",
    ")\n",
    "val_episode_dataset = EpisodeDataset(\n",
    "    dataset,\n",
    "    n_obs_history=n_obs_history,\n",
    "    n_pred_horizon=n_pred_horizon,\n",
    "    episode_idxs=val_idxs,\n",
    "    process_img_fn=process_img_fn,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_episode_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_episode_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "source": [
    "obs_history, pred_horizon = next(iter(train_loader))\n",
    "for k, v in obs_history.items():\n",
    "    print(f\"obs_history['{k}']\", v.shape)\n",
    "for k, v in pred_horizon.items():\n",
    "    print(f\"pred_horizon['{k}']\", v.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "class InverseDynamicsMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_obs: int, obs_dim: int, action_dim: int, hidden_dims: list[int]\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_dim = n_obs * obs_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, action_dim))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, obs_history: torch.Tensor) -> torch.Tensor:\n",
    "        x = obs_history.flatten(start_dim=1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "class InverseDynamicsCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        in_height: int,\n",
    "        in_width: int,\n",
    "        action_dim: int,\n",
    "        n_obs_history: int,\n",
    "        hidden_dims: list[int] = None,\n",
    "    ):\n",
    "        super(InverseDynamicsCNN, self).__init__()\n",
    "\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        modules = []\n",
    "        kernel_size = 3\n",
    "        stride = 2\n",
    "        padding = 1\n",
    "        dilation = 1\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        out_channels=h_dim,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=padding,\n",
    "                        dilation=dilation,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        # Define the initial part of the CNN that processes individual images\n",
    "        self.conv_branch = nn.Sequential(\n",
    "            *modules,\n",
    "            # Flatten the output for the dense layers\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute the shape of the output of the convolutional branch before it\n",
    "        # is flattened and passed through the dense layers.\n",
    "        conv_out_shape = compute_conv_output_shape(\n",
    "            H=in_height,\n",
    "            W=in_width,\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            num_layers=len(hidden_dims),\n",
    "            last_hidden_dim=hidden_dims[-1],\n",
    "        )\n",
    "        conv_out_size = np.prod(conv_out_shape)\n",
    "\n",
    "        # Define the part of the network that combines features and predicts the action\n",
    "        self.action_predictor = nn.Sequential(\n",
    "            nn.Linear(n_obs_history * conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs_history: torch.Tensor):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            obs_history (torch.Tensor): Observation history of shape (batch_size, n_obs, C, H, W).\n",
    "        \"\"\"\n",
    "        # List to hold the features extracted from each image in the observation history\n",
    "        features_list = []\n",
    "\n",
    "        # Iterate over the second dimension (n_obs) of the obs_history tensor\n",
    "        for i in range(obs_history.size(1)):\n",
    "            # Extract the i-th image from the observation history\n",
    "            img_i = obs_history[:, i]\n",
    "\n",
    "            # Process the image through the convolutional branch\n",
    "            img_i_features = self.conv_branch(img_i)\n",
    "\n",
    "            # Append the features to the list\n",
    "            features_list.append(img_i_features)\n",
    "\n",
    "        # Concatenate the features from all images along the feature dimension (dim=1)\n",
    "        combined_features = torch.cat(features_list, dim=1)\n",
    "\n",
    "        # Predict the action from the combined features\n",
    "        action_pred = self.action_predictor(combined_features)\n",
    "        return action_pred"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "source": [
    "# Sanity check image normalization\n",
    "img = dataset.replay_buffer.get_episode(0)[\"img\"].transpose(0, 3, 1, 2)\n",
    "encoded_img = encode_images(img, img_normalizer, vae, device)\n",
    "print(\"img.shape:\", img.shape)\n",
    "print(\"encoded_img.shape:\", encoded_img.shape)\n",
    "\n",
    "print(\"encoded_img.mean():\", encoded_img.mean())\n",
    "print(\"encoded_img.std():\", encoded_img.std())\n",
    "normalized_encoded_img = latent_img_normalizer(encoded_img)\n",
    "print(\"normalized_encoded_img.mean():\", normalized_encoded_img.mean())\n",
    "print(\"normalized_encoded_img.std():\", normalized_encoded_img.std())\n",
    "\n",
    "# Sanity check action normalization\n",
    "action = dataset.replay_buffer.get_episode(0)[\"action\"]\n",
    "normalized_action = action_normalizer(action)\n",
    "print(\"action.min():\", action.min())\n",
    "print(\"action.max():\", action.max())\n",
    "print(\"normalized_action.min():\", normalized_action.min())\n",
    "print(\"normalized_action.max():\", normalized_action.max())\n",
    "\n",
    "# Sanity check state normalization\n",
    "state = dataset.replay_buffer.get_episode(0)[\"state\"]\n",
    "state_normalizer = functools.partial(normalize_state, min_state=min_state, max_state=max_state)\n",
    "normalized_state = state_normalizer(state)\n",
    "print(\"state.min():\", state.min(axis=0))\n",
    "print(\"state.max():\", state.max(axis=0))\n",
    "print(\"normalized_state.min():\", normalized_state.min(axis=0))\n",
    "print(\"normalized_state.max():\", normalized_state.max(axis=0))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "source": [
    "def train_epochs(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    obs_normalizer,\n",
    "    action_normalizer,\n",
    "    obs_key: str = \"img\",\n",
    "    opt_kwargs: Optional[dict] = None,\n",
    "    num_epochs=10,\n",
    "    log_freq: Optional[int] = None,\n",
    "    save_freq=2,\n",
    "    save_dir: Optional[str] = None,\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    criterion = nn.MSELoss()\n",
    "    opt_kwargs = opt_kwargs or {}\n",
    "    optimizer = torch.optim.Adam(model.parameters(), **opt_kwargs)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = [\n",
    "        eval(\n",
    "            model,\n",
    "            val_loader,\n",
    "            obs_normalizer,\n",
    "            action_normalizer,\n",
    "            criterion,\n",
    "            device,\n",
    "            obs_key=obs_key,\n",
    "        )\n",
    "    ]\n",
    "    with trange(num_epochs, desc=\"Epoch\") as tepoch:\n",
    "        for epoch in tepoch:\n",
    "            model.train()\n",
    "            with tqdm(train_loader, desc=\"Batch\") as tbatch:\n",
    "                # Prediction horizon is unused.\n",
    "                for i, (obs_history, _) in enumerate(tbatch):\n",
    "                    obs = obs_history[obs_key]\n",
    "                    # The second-to-last action is the target action because it was\n",
    "                    # applied to get the last image.\n",
    "                    action = obs_history[\"action\"][:, -2]\n",
    "\n",
    "                    # Normalize image and action.\n",
    "                    obs = obs_normalizer(obs)\n",
    "                    action = action_normalizer(action)\n",
    "                    # assert obs.min() >= -1 and obs.max() <= 1\n",
    "                    # assert action.min() >= -1 and action.max() <= 1\n",
    "\n",
    "                    obs = obs.to(device)\n",
    "                    action = action.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    action_pred = model(obs)\n",
    "                    loss = criterion(action_pred, action)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    loss_cpu = loss.item()\n",
    "                    train_losses.append(loss_cpu)\n",
    "\n",
    "                    tbatch.set_postfix(loss=loss_cpu)\n",
    "                    if log_freq is not None and (i % log_freq == 0):\n",
    "                        print(f\"Epoch {epoch}, Batch {i}, Train Loss: {loss_cpu}\")\n",
    "\n",
    "            # Eval\n",
    "            test_loss = eval(\n",
    "                model,\n",
    "                val_loader,\n",
    "                obs_normalizer,\n",
    "                action_normalizer,\n",
    "                criterion,\n",
    "                device,\n",
    "                obs_key=obs_key,\n",
    "            )\n",
    "            test_losses.append(test_loss)\n",
    "            tepoch.set_postfix(test_loss=test_loss)\n",
    "\n",
    "            # Save\n",
    "            if save_dir is not None and (\n",
    "                epoch % save_freq == 0 or epoch == num_epochs - 1\n",
    "            ):\n",
    "                epoch_str = \"final\" if epoch == num_epochs - 1 else str(epoch)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(save_dir, f\"inverse_dynamics_{epoch_str}.pt\"),\n",
    "                )\n",
    "\n",
    "    return train_losses, test_losses\n",
    "\n",
    "\n",
    "def eval(\n",
    "    model,\n",
    "    val_loader,\n",
    "    obs_normalizer,\n",
    "    action_normalizer,\n",
    "    criterion,\n",
    "    device,\n",
    "    obs_key: str = \"img\",\n",
    "):\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for obs_history, _ in val_loader:\n",
    "            obs = obs_history[obs_key]\n",
    "            # The second-to-last action is the target action because it was\n",
    "            # applied to get the last image.\n",
    "            action = obs_history[\"action\"][:, -2]\n",
    "\n",
    "            obs = obs_normalizer(obs).to(device)\n",
    "            action = action_normalizer(action).to(device)\n",
    "\n",
    "            action_pred = model(obs)\n",
    "            loss = criterion(action_pred, action)\n",
    "            # Multiply the loss by the number of samples in the batch.\n",
    "            test_losses.append(loss.item() * obs.shape[0])\n",
    "\n",
    "    # Compute the average loss across all batches.\n",
    "    test_loss = np.sum(test_losses) / len(val_loader.dataset)\n",
    "    return test_loss\n",
    "\n",
    "\n",
    "def plot_losses(train_losses, test_losses):\n",
    "    # Plot train and test losses.\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.plot(\n",
    "        np.linspace(0, len(train_losses), len(test_losses)),\n",
    "        test_losses,\n",
    "        label=\"Test Loss\",\n",
    "    )\n",
    "    # Remove outliers for better visualization\n",
    "    # plt.ylim(0, 0.01)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State history to action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "source": [
    "# Make train and val loaders\n",
    "# val_mask = get_val_mask(dataset.replay_buffer.n_episodes, 0.1)\n",
    "# val_idxs = np.where(val_mask)[0]\n",
    "# train_idxs = np.where(~val_mask)[0]\n",
    "\n",
    "# Debug train/validation split\n",
    "train_idxs = [0]\n",
    "val_idxs = [1]\n",
    "\n",
    "# Make the episode dataset and create a DataLoader.\n",
    "batch_size = 256\n",
    "n_obs_history = 10\n",
    "n_pred_horizon = 0\n",
    "process_img_fn = None\n",
    "train_episode_dataset = EpisodeDataset(\n",
    "    dataset,\n",
    "    n_obs_history=n_obs_history,\n",
    "    n_pred_horizon=n_pred_horizon,\n",
    "    episode_idxs=train_idxs,\n",
    "    process_img_fn=process_img_fn,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "val_episode_dataset = EpisodeDataset(\n",
    "    dataset,\n",
    "    n_obs_history=n_obs_history,\n",
    "    n_pred_horizon=n_pred_horizon,\n",
    "    episode_idxs=val_idxs,\n",
    "    process_img_fn=process_img_fn,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_episode_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_episode_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "state_normalizer = functools.partial(\n",
    "    normalize_pn1,\n",
    "    min_val=torch.tensor(min_state, dtype=torch.float32).to(device),\n",
    "    max_val=torch.tensor(max_state, dtype=torch.float32).to(device),\n",
    ")\n",
    "action_normalizer = functools.partial(\n",
    "    normalize_pn1,\n",
    "    min_val=torch.tensor(min_action, dtype=torch.float32).to(device),\n",
    "    max_val=torch.tensor(max_action, dtype=torch.float32).to(device),\n",
    ")\n",
    "# obs_dim is the state dimension.\n",
    "id_model = InverseDynamicsMLP(\n",
    "    n_obs=n_obs_history, obs_dim=5, action_dim=2, hidden_dims=[256, 256, 256]\n",
    ").to(device)\n",
    "train_losses, test_losses = train_epochs(\n",
    "    id_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    state_normalizer,\n",
    "    action_normalizer,\n",
    "    obs_key=\"state\",\n",
    "    opt_kwargs={\"lr\": 1e-3, \"weight_decay\": 0},\n",
    "    num_epochs=100,\n",
    "    log_freq=None,\n",
    "    save_freq=2,\n",
    "    save_dir=None,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "source": [
    "plot_losses(train_losses, test_losses)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "source": [
    "batch = next(iter(val_loader))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "source": [
    "eval(\n",
    "    id_model,\n",
    "    val_loader,\n",
    "    state_normalizer,\n",
    "    action_normalizer,\n",
    "    nn.MSELoss(),\n",
    "    device,\n",
    "    obs_key=\"state\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "source": [
    "states = batch[0][\"state\"]\n",
    "norm_states = state_normalizer(states)\n",
    "\n",
    "actions = batch[0][\"action\"]\n",
    "target_action = actions[:, -2]\n",
    "norm_actions = action_normalizer(actions)\n",
    "norm_target_action = norm_actions[:, -2]\n",
    "\n",
    "norm_pred_action = id_model(norm_states)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "source": [
    "actions.shape, target_action.shape, norm_target_action.shape, norm_pred_action.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "source": [
    "F.mse_loss(norm_pred_action, norm_target_action, reduction=\"mean\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "source": [
    "F.mse_loss(\n",
    "    denormalize_pn1(\n",
    "        norm_pred_action,\n",
    "        torch.tensor(min_action).cuda(),\n",
    "        torch.tensor(max_action).cuda(),\n",
    "    ),\n",
    "    target_action,\n",
    "    reduction=\"mean\",\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make train and val loaders\n",
    "val_mask = get_val_mask(dataset.replay_buffer.n_episodes, 0.1)\n",
    "val_idxs = np.where(val_mask)[0]\n",
    "train_idxs = np.where(~val_mask)[0]\n",
    "\n",
    "# Make the episode dataset and create a DataLoader.\n",
    "batch_size = 256\n",
    "n_obs_history = 5\n",
    "n_pred_horizon = 0\n",
    "process_img_fn = functools.partial(\n",
    "    encode_images, img_normalizer=img_normalizer, vae=vae, device=device\n",
    ")\n",
    "train_episode_dataset = EpisodeDataset(\n",
    "    dataset,\n",
    "    n_obs_history=n_obs_history,\n",
    "    n_pred_horizon=n_pred_horizon,\n",
    "    episode_idxs=train_idxs,\n",
    "    process_img_fn=process_img_fn,\n",
    ")\n",
    "val_episode_dataset = EpisodeDataset(\n",
    "    dataset,\n",
    "    n_obs_history=n_obs_history,\n",
    "    n_pred_horizon=n_pred_horizon,\n",
    "    episode_idxs=val_idxs,\n",
    "    process_img_fn=process_img_fn,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_episode_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_episode_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "id_model = InverseDynamicsMLP(\n",
    "    n_obs=n_obs_history, obs_dim=32, action_dim=2, hidden_dims=[256, 256, 256]\n",
    ").to(device)\n",
    "train_losses, test_losses = train_epochs(\n",
    "    id_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    latent_img_normalizer,\n",
    "    action_normalizer,\n",
    "    opt_kwargs={\"lr\": 1e-3, \"weight_decay\": 1e-5},\n",
    "    num_epochs=10,\n",
    "    log_freq=None,\n",
    "    save_freq=2,\n",
    "    save_dir=None,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "plot_losses(train_losses, test_losses)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "source": [
    "# Make train and val loaders\n",
    "val_mask = get_val_mask(dataset.replay_buffer.n_episodes, 0.1)\n",
    "val_idxs = np.where(val_mask)[0]\n",
    "train_idxs = np.where(~val_mask)[0]\n",
    "\n",
    "# Make the episode dataset and create a DataLoader.\n",
    "batch_size = 256\n",
    "n_obs_history = 2\n",
    "n_pred_horizon = 0\n",
    "process_img_fn = lambda img: img / 255.0\n",
    "train_episode_dataset = EpisodeDataset(\n",
    "    dataset,\n",
    "    n_obs_history=n_obs_history,\n",
    "    n_pred_horizon=n_pred_horizon,\n",
    "    episode_idxs=train_idxs,\n",
    "    process_img_fn=process_img_fn,\n",
    ")\n",
    "val_episode_dataset = EpisodeDataset(\n",
    "    dataset,\n",
    "    n_obs_history=n_obs_history,\n",
    "    n_pred_horizon=n_pred_horizon,\n",
    "    episode_idxs=val_idxs,\n",
    "    process_img_fn=process_img_fn,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_episode_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_episode_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_dims = None\n",
    "N, H, W, C = dataset.replay_buffer[\"img\"].shape\n",
    "N, action_dim = dataset.replay_buffer[\"action\"].shape\n",
    "cnn_id_model = InverseDynamicsCNN(\n",
    "    C, H, W, action_dim, n_obs_history, hidden_dims=hidden_dims\n",
    ").to(device)\n",
    "img_normalizer = get_image_range_normalizer()\n",
    "train_losses, test_losses = train_epochs(\n",
    "    cnn_id_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    img_normalizer,\n",
    "    action_normalizer,\n",
    "    opt_kwargs={\"lr\": 1e-4},\n",
    "    num_epochs=10,\n",
    "    log_freq=None,\n",
    "    save_freq=2,\n",
    "    save_dir=None,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "source": [
    "plot_losses(train_losses, test_losses)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "source": [
    "# Make train and val loaders\n",
    "val_mask = get_val_mask(dataset.replay_buffer.n_episodes, 0.1)\n",
    "val_idxs = np.where(val_mask)[0]\n",
    "train_idxs = np.where(~val_mask)[0]\n",
    "\n",
    "# Make the episode dataset and create a DataLoader.\n",
    "batch_size = 256\n",
    "n_obs_history = 4\n",
    "n_pred_horizon = 0\n",
    "process_img_fn = lambda img: img / 255.0\n",
    "train_episode_dataset = EpisodeDataset(\n",
    "    dataset,\n",
    "    n_obs_history=n_obs_history,\n",
    "    n_pred_horizon=n_pred_horizon,\n",
    "    episode_idxs=train_idxs,\n",
    "    process_img_fn=process_img_fn,\n",
    ")\n",
    "val_episode_dataset = EpisodeDataset(\n",
    "    dataset,\n",
    "    n_obs_history=n_obs_history,\n",
    "    n_pred_horizon=n_pred_horizon,\n",
    "    episode_idxs=val_idxs,\n",
    "    process_img_fn=process_img_fn,\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_episode_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_episode_dataset, batch_size=batch_size, shuffle=False\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_dims = None\n",
    "N, H, W, C = dataset.replay_buffer[\"img\"].shape\n",
    "N, action_dim = dataset.replay_buffer[\"action\"].shape\n",
    "cnn_id_model = InverseDynamicsCNN(\n",
    "    C, H, W, action_dim, n_obs_history, hidden_dims=hidden_dims\n",
    ").to(device)\n",
    "img_normalizer = get_image_range_normalizer()\n",
    "train_losses, test_losses = train_epochs(\n",
    "    cnn_id_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    img_normalizer,\n",
    "    action_normalizer,\n",
    "    opt_kwargs={\"lr\": 1e-4},\n",
    "    num_epochs=10,\n",
    "    log_freq=None,\n",
    "    save_freq=2,\n",
    "    save_dir=None,\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "source": [
    "plot_losses(train_losses, test_losses)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "source": [
    "# Evaluate the model on the validation set.\n",
    "test_loss = eval(model, val_loader, criterion, device)\n",
    "print(\"Final Test Loss:\", test_loss)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "source": [
    "# Get the true and predicted action for a test batch.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    cur_img, next_img, action = next(iter(val_loader))\n",
    "    cur_img = img_normalizer(cur_img)\n",
    "    next_img = img_normalizer(next_img)\n",
    "    action = action_normalizer(action)\n",
    "\n",
    "    cur_img = cur_img.to(device)\n",
    "    next_img = next_img.to(device)\n",
    "    action = action.to(device)\n",
    "\n",
    "    action_pred = model(cur_img, next_img)\n",
    "\n",
    "# Unnormalize the action prediction\n",
    "unnormalized_action = action_normalizer.unnormalize(action)\n",
    "unnormalized_action_pred = action_normalizer.unnormalize(action_pred)\n",
    "normalized_mse = criterion(action, action_pred)\n",
    "unnormalized_mse = criterion(unnormalized_action, unnormalized_action_pred)\n",
    "\n",
    "print(\"Normalized MSE:\", normalized_mse.item())\n",
    "print(\"Unnormalized MSE:\", unnormalized_mse.item())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "source": [
    "(unnormalized_action[:5], unnormalized_action_pred[:5])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "# Load the model.\n",
    "model = InverseDynamicsCNN(C, H, W, action_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"inverse_dynamics_cnn_final.pt\"))"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
