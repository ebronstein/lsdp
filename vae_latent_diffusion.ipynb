{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36a3146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Optional\n",
    "\n",
    "if \"PyTorch_VAE\" not in sys.path:\n",
    "    sys.path.append(\"PyTorch_VAE\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "from PyTorch_VAE import models\n",
    "from diffusion_policy.common.pytorch_util import compute_conv_output_shape\n",
    "from diffusion_policy.dataset.pusht_image_dataset import PushTImageDataset\n",
    "from vae.pusht_vae import VanillaVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0872279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/nas/ucb/ebronstein/lsdp/diffusion_policy/data/pusht/pusht_cchi_v7_replay.zarr\"\n",
    "# path = \"/home/tsadja/data_diffusion/pusht/pusht_cchi_v7_replay.zarr\"\n",
    "path = '/home/matteogu/ssd_data/data_diffusion/pusht/pusht_cchi_v7_replay.zarr'\n",
    "\n",
    "dataset = PushTImageDataset(path)\n",
    "full_dataset = torch.from_numpy(dataset.replay_buffer[\"img\"]).permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79ec2083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    data /= 255.0\n",
    "    data = 2 * data - 1\n",
    "    return data\n",
    "\n",
    "\n",
    "def unnormalize(data):\n",
    "    data = (data + 1) / 2\n",
    "    data *= 255\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce31b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaVAE(models.VanillaVAE):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        in_height: int,\n",
    "        in_width: int,\n",
    "        latent_dim: int,\n",
    "        hidden_dims: Optional[list] = None,\n",
    "        **kwargs\n",
    "    ) -> None:\n",
    "        models.BaseVAE.__init__(self)\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        kernel_size = 3\n",
    "        stride = 2\n",
    "        padding = 1\n",
    "        dilation = 1\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        out_channels=h_dim,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=padding,\n",
    "                        dilation=dilation,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.conv_out_shape = compute_conv_output_shape(\n",
    "            H=in_height,\n",
    "            W=in_width,\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            num_layers=len(hidden_dims),\n",
    "            last_hidden_dim=hidden_dims[-1],\n",
    "        )\n",
    "        conv_out_size = np.prod(self.conv_out_shape)\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(conv_out_size, latent_dim)\n",
    "        self.fc_var = nn.Linear(conv_out_size, latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        self.decoder_input = nn.Linear(latent_dim, conv_out_size)\n",
    "\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(\n",
    "                        hidden_dims[i],\n",
    "                        hidden_dims[i + 1],\n",
    "                        kernel_size=3,\n",
    "                        stride=2,\n",
    "                        padding=1,\n",
    "                        output_padding=1,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                hidden_dims[-1],\n",
    "                hidden_dims[-1],\n",
    "                kernel_size=3,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                output_padding=1,\n",
    "            ),\n",
    "            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Conv2d(hidden_dims[-1], out_channels=3, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1, *self.conv_out_shape)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3271409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vae.pusht_vae import VanillaVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d2fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = normalize(full_dataset)\n",
    "N, C, H, W = full_dataset.shape\n",
    "train_split = 0.8\n",
    "train_size = int(train_split * N)\n",
    "val_size = N - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, val_size]\n",
    ")\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ba95ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VanillaVAE(in_channels=3, in_height=H, in_width=W, latent_dim=latent_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa109673",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in trange(epochs):\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for i, x in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        result = model(x)\n",
    "        loss = model.loss_function(*result, M_N=1e-6)[\"loss\"]\n",
    "        # loss = loss['loss']\n",
    "        total_train_loss += loss.item()\n",
    "        train_losses.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Train loss: {total_train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    total_val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, x in enumerate(val_loader):\n",
    "            x = x.to(device)\n",
    "            result = model(x)\n",
    "            loss = model.loss_function(*result, M_N=1e-6)[\"loss\"]\n",
    "            total_val_loss += loss.item()\n",
    "    val_losses.append(total_val_loss / len(val_loader))\n",
    "    print(f\"Validation loss: {val_losses[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ed7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_reconstructions(model: VanillaVAE, val_loader: torch.utils.data.DataLoader, save_fig: bool = False):\n",
    "    val_data = next(iter(val_loader))\n",
    "    num_samples = 5\n",
    "    val_data = val_data.to(device)\n",
    "    result = model(val_data)\n",
    "    recon = result[0]\n",
    "    recon = unnormalize(recon)\n",
    "    val_data = unnormalize(val_data)\n",
    "\n",
    "    fig, ax = plt.subplots(2, num_samples, figsize=(num_samples*2, 6))\n",
    "    # fig.set_size_inches(10, 10)\n",
    "    for ii in range(num_samples):\n",
    "        ax[0, ii].imshow(\n",
    "            val_data[ii].permute(1, 2, 0).cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "        ax[1, ii].imshow(\n",
    "            recon[ii].permute(1, 2, 0).cpu().detach().numpy().astype(np.uint8)\n",
    "        )\n",
    "        ax[0, ii].axis('off')\n",
    "        ax[1, ii].axis('off')\n",
    "        \n",
    "    # plt.suptitle(\"Reconstructions\")\n",
    "    ax[0, 0].set_title('Ground Truth')\n",
    "    ax[1, 0].set_title('Reconstruction')\n",
    "    plt.tight_layout()\n",
    "    if save_fig: plt.savefig(f'figs/pusht_vae/reconstructions_{latent_dim}.png')\n",
    "    plt.show()\n",
    "    \n",
    "show_reconstructions(model, val_loader, save_fig=True)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plt.plot(train_losses)\n",
    "# plt.plot(val_losses)\n",
    "def plot_losses(train_losses, test_losses):\n",
    "    # Plot train and test losses.\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_losses, label=\"Train Loss\")\n",
    "    plt.semilogy(\n",
    "        np.linspace(0, len(train_losses), len(test_losses)),\n",
    "        test_losses,\n",
    "        label=\"Test Loss\",\n",
    "    )\n",
    "    # Remove outliers for better visualization\n",
    "    # plt.ylim(0, 0.01)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(f'[Latent {latent_dim}] Final Test loss: {test_losses[-1]:.4f}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "plot_losses(train_losses, val_losses)"
   ],
   "id": "183cac383a831761",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773f4dd2178d008",
   "metadata": {},
   "source": [
    "with open(f'{save_dir}/losses/losses_{latent_dim}_{now}.npy', 'rb') as f:\n",
    "    train_losses_l = np.load(f)\n",
    "    val_losses_l = np.load(f)\n"
   ],
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plots for the report",
   "id": "bdd3a73e5f870099"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os \n",
    "loss_path = 'models/pusht_vae/losses/'\n",
    "exps = os.listdir(loss_path)\n",
    "exps.sort()\n",
    "plt.figure(figsize=(12, 6))\n",
    "for exp in exps:\n",
    "    with open(f'{loss_path}{exp}', 'rb') as f:\n",
    "        _train_losses_ = np.load(f)\n",
    "        _val_losses = np.load(f)\n",
    "    \n",
    "    _latent_dim = int(exp.split('_')[1])\n",
    "    plt.semilogy(_train_losses_, label=f\"[{_latent_dim}] Train Loss\")    \n",
    "    plt.semilogy(\n",
    "        np.linspace(0, len(_train_losses_), len(_val_losses)),\n",
    "        _val_losses,\n",
    "        label=f\"[{_latent_dim}] Test Loss, final_value: {_val_losses[-1]:.4f}\",\n",
    "    )\n",
    "    # Remove outliers for better visualization\n",
    "    # plt.ylim(0, 0.01)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(f'Test loss vs Latent Dimension')\n",
    "plt.tight_layout()\n",
    "plt.savefig('Loss_vs_latent.png')\n",
    "plt.show()"
   ],
   "id": "1c734d4f3f852974",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "837db8f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_dim = 32\n",
    "# Load the VAE\n",
    "model = VanillaVAE(in_channels=3, in_height=H, in_width=W, latent_dim=latent_dim).to(device)\n",
    "save_dir = \"models/pusht_vae\"\n",
    "model.load_state_dict(torch.load(os.path.join(save_dir, \"vae_32_20240403.pt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a58717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the full dataset\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    mu, log_var = model.encode(full_dataset.to(device))\n",
    "    mu = mu.cpu().detach().numpy()\n",
    "    log_var = log_var.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61dccab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25650, 32), (25650, 32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.shape, log_var.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
