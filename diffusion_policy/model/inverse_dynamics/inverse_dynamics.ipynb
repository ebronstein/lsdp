{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from diffusion_policy.common.normalize_util import get_image_range_normalizer\n",
    "from diffusion_policy.common.pytorch_util import compute_conv_output_shape\n",
    "from diffusion_policy.common.sampler import get_val_mask\n",
    "from diffusion_policy.dataset.pusht_image_dataset import PushTImageDataset\n",
    "from diffusion_policy.model.common.normalizer import LinearNormalizer"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "path = \"/nas/ucb/ebronstein/lsdp/diffusion_policy/data/pusht/pusht_cchi_v7_replay.zarr\"\n",
    "dataset = PushTImageDataset(path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "episode = dataset.replay_buffer.get_episode(0)\n",
    "episode.keys()\n",
    "print(\"episode['img']:\", episode['img'].shape)\n",
    "print(\"episode['action']:\", episode['action'].shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "episode_len = episode[\"img\"].shape[0]\n",
    "time_steps = np.linspace(0, episode_len - 1, 25).astype(int)\n",
    "# Plot the first few images and actions in the episode\n",
    "fig, axs = plt.subplots(5, 5, figsize=(20, 20))\n",
    "for i, step in enumerate(time_steps):\n",
    "    ax = axs[i // 5, i % 5]\n",
    "    ax.imshow(episode[\"img\"][step] / 255.0)\n",
    "    ax.set_title(episode[\"action\"][step])\n",
    "    ax.axis(\"off\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "source": [
    "class EpisodeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, episode_idxs=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with the main dataset object that contains\n",
    "        the replay_buffer.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.episode_idxs = episode_idxs\n",
    "        self.prepare_data()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Preprocess the episodes to create a flat list of samples.\n",
    "        Each sample is a tuple: (current_image, next_image, action).\n",
    "        \"\"\"\n",
    "        self.samples = []\n",
    "\n",
    "        if self.episode_idxs is None:\n",
    "            self.episode_idxs = range(self.dataset.replay_buffer.n_episodes)\n",
    "\n",
    "        for episode_idx in self.episode_idxs:\n",
    "            episode = self.dataset.replay_buffer.get_episode(episode_idx)\n",
    "            img = episode[\"img\"] / 255.0  # Normalize the images to [0, 1]\n",
    "            actions = episode[\"action\"]\n",
    "\n",
    "            # Ensure there is a next image for each current image\n",
    "            assert len(img) == len(actions)\n",
    "\n",
    "            for i in range(len(actions) - 1):\n",
    "                self.samples.append((img[i], img[i + 1], actions[i]))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of samples across all episodes.\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return the idx-th sample from the dataset.\n",
    "        \"\"\"\n",
    "        current_img, next_img, action = self.samples[idx]\n",
    "\n",
    "        # Convert data to PyTorch tensors and ensure the data type is correct\n",
    "        current_img_tensor = torch.tensor(current_img, dtype=torch.float32).permute(\n",
    "            2, 0, 1\n",
    "        )  # Convert HWC to CHW\n",
    "        next_img_tensor = torch.tensor(next_img, dtype=torch.float32).permute(\n",
    "            2, 0, 1\n",
    "        )  # Convert HWC to CHW\n",
    "        action_tensor = torch.tensor(action, dtype=torch.float32)\n",
    "\n",
    "        return current_img_tensor, next_img_tensor, action_tensor"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "source": [
    "val_mask = get_val_mask(dataset.replay_buffer.n_episodes, 0.1)\n",
    "val_idxs = np.where(val_mask)[0]\n",
    "train_idxs = np.where(~val_mask)[0]\n",
    "\n",
    "# Make the episode dataset and create a DataLoader.\n",
    "batch_size = 256\n",
    "train_episode_dataset = EpisodeDataset(dataset, episode_idxs=train_idxs)\n",
    "val_episode_dataset = EpisodeDataset(dataset, episode_idxs=val_idxs)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_episode_dataset, batch_size=batch_size, shuffle=True, num_workers=1\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_episode_dataset, batch_size=batch_size, shuffle=False, num_workers=1\n",
    ")\n",
    "\n",
    "cur_img, next_img, action = next(iter(train_loader))\n",
    "print(\"cur_img.shape:\", cur_img.shape)\n",
    "print(\"next_img.shape:\", next_img.shape)\n",
    "print(\"action.shape:\", action.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# Make the action and image normalizers.\n",
    "action_normalizer = LinearNormalizer()\n",
    "action_normalizer.fit(\n",
    "    data=dataset.replay_buffer[\"action\"], last_n_dims=1, mode=\"limits\"\n",
    ")\n",
    "img_normalizer = get_image_range_normalizer()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# Normalize image and action to [-1, 1].\n",
    "cur_img_normalized = img_normalizer(cur_img)\n",
    "next_img_normalized = img_normalizer(next_img)\n",
    "action_normalized = action_normalizer(action)\n",
    "\n",
    "assert cur_img_normalized.min() >= -1 and cur_img_normalized.max() <= 1\n",
    "assert next_img_normalized.min() >= -1 and next_img_normalized.max() <= 1\n",
    "assert action_normalized.min() >= -1 and action_normalized.max() <= 1"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "source": [
    "class InverseDynamicsCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        in_height: int,\n",
    "        in_width: int,\n",
    "        action_dim: int,\n",
    "        hidden_dims: list[int] = None,\n",
    "    ):\n",
    "        super(InverseDynamicsCNN, self).__init__()\n",
    "\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        modules = []\n",
    "        kernel_size = 3\n",
    "        stride = 2\n",
    "        padding = 1\n",
    "        dilation = 1\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        in_channels,\n",
    "                        out_channels=h_dim,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=padding,\n",
    "                        dilation=dilation,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU(),\n",
    "                )\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        # Define the initial part of the CNN that processes individual images\n",
    "        self.conv_branch = nn.Sequential(\n",
    "            *modules,\n",
    "            # Flatten the output for the dense layers\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute the shape of the output of the convolutional branch before it\n",
    "        # is flattened and passed through the dense layers.\n",
    "        conv_out_shape = compute_conv_output_shape(\n",
    "            H=in_height,\n",
    "            W=in_width,\n",
    "            padding=padding,\n",
    "            stride=stride,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            num_layers=len(hidden_dims),\n",
    "            last_hidden_dim=hidden_dims[-1],\n",
    "        )\n",
    "        conv_out_size = np.prod(conv_out_shape)\n",
    "\n",
    "        # Define the part of the network that combines features and predicts the action\n",
    "        self.action_predictor = nn.Sequential(\n",
    "            nn.Linear(2 * conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, image1, image2):\n",
    "        # Process each image through the same convolutional branch\n",
    "        img1_features = self.conv_branch(image1)\n",
    "        img2_features = self.conv_branch(image2)\n",
    "\n",
    "        # Concatenate the features from both images\n",
    "        combined_features = torch.cat((img1_features, img2_features), dim=1)\n",
    "\n",
    "        # Predict the action from the combined features\n",
    "        action_pred = self.action_predictor(combined_features)\n",
    "        return action_pred"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "source": [
    "def eval(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    with torch.no_grad():\n",
    "        for cur_img, next_img, action in val_loader:\n",
    "            cur_img = img_normalizer(cur_img)\n",
    "            next_img = img_normalizer(next_img)\n",
    "            action = action_normalizer(action)\n",
    "\n",
    "            cur_img = cur_img.to(device)\n",
    "            next_img = next_img.to(device)\n",
    "            action = action.to(device)\n",
    "\n",
    "            action_pred = model(cur_img, next_img)\n",
    "            loss = criterion(action_pred, action)\n",
    "            # Multiply the loss by the number of samples in the batch.\n",
    "            test_losses.append(loss.item() * len(cur_img))\n",
    "\n",
    "    # Compute the average loss across all batches.\n",
    "    test_loss = np.sum(test_losses) / len(val_loader.dataset)\n",
    "    return test_loss"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "source": [
    "N, H, W, C = dataset.replay_buffer[\"img\"].shape\n",
    "N, action_dim = dataset.replay_buffer[\"action\"].shape\n",
    "\n",
    "val_mask = get_val_mask(dataset.replay_buffer.n_episodes, 0.1)\n",
    "val_idxs = np.where(val_mask)[0]\n",
    "train_idxs = np.where(~val_mask)[0]\n",
    "\n",
    "# Make the episode dataset and create a DataLoader.\n",
    "batch_size = 256\n",
    "train_episode_dataset = EpisodeDataset(dataset, episode_idxs=train_idxs)\n",
    "val_episode_dataset = EpisodeDataset(dataset, episode_idxs=val_idxs)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_episode_dataset, batch_size=batch_size, shuffle=True, num_workers=1\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_episode_dataset, batch_size=batch_size, shuffle=False, num_workers=1\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "hidden_dims = None\n",
    "model = InverseDynamicsCNN(C, H, W, action_dim, hidden_dims=hidden_dims).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10\n",
    "log_freq = 10\n",
    "save_freq = 2\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for epoch in trange(num_epochs, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    for i, (cur_img, next_img, action) in enumerate(\n",
    "        tqdm(train_loader, desc=\"Batch\", leave=False)\n",
    "    ):\n",
    "        # Normalize image and action to [-1, 1].\n",
    "        cur_img = img_normalizer(cur_img)\n",
    "        next_img = img_normalizer(next_img)\n",
    "        action = action_normalizer(action)\n",
    "\n",
    "        cur_img = cur_img.to(device)\n",
    "        next_img = next_img.to(device)\n",
    "        action = action.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        action_pred = model(cur_img, next_img)\n",
    "        loss = criterion(action_pred, action)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if i % log_freq == 0:\n",
    "            print(f\"Epoch {epoch}, Batch {i}, Train Loss: {loss.item()}\")\n",
    "\n",
    "    # Eval\n",
    "    test_loss = eval(model, val_loader, criterion, device)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"Epoch {epoch}, Test Loss: {test_loss}\")\n",
    "\n",
    "    # Save\n",
    "    if epoch % save_freq == 0 or epoch == num_epochs - 1:\n",
    "        epoch_str = \"final\" if epoch == num_epochs - 1 else str(epoch)\n",
    "        torch.save(model.state_dict(), f\"inverse_dynamics_cnn_{epoch_str}.pt\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "source": [
    "# Plot train and test losses.\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(np.linspace(0, len(train_losses), len(test_losses)), test_losses, label=\"Test Loss\")\n",
    "# Remove outliers for better visualization\n",
    "plt.ylim(0, 0.01)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "source": [
    "# Evaluate the model on the validation set.\n",
    "test_loss = eval(model, val_loader, criterion, device)\n",
    "print(\"Final Test Loss:\", test_loss)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "source": [
    "# Get the true and predicted action for a test batch.\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    cur_img, next_img, action = next(iter(val_loader))\n",
    "    cur_img = img_normalizer(cur_img)\n",
    "    next_img = img_normalizer(next_img)\n",
    "    action = action_normalizer(action)\n",
    "\n",
    "    cur_img = cur_img.to(device)\n",
    "    next_img = next_img.to(device)\n",
    "    action = action.to(device)\n",
    "\n",
    "    action_pred = model(cur_img, next_img)\n",
    "\n",
    "# Unnormalize the action prediction\n",
    "unnormalized_action = action_normalizer.unnormalize(action)\n",
    "unnormalized_action_pred = action_normalizer.unnormalize(action_pred)\n",
    "normalized_mse = criterion(action, action_pred)\n",
    "unnormalized_mse = criterion(unnormalized_action, unnormalized_action_pred)\n",
    "\n",
    "print(\"Normalized MSE:\", normalized_mse.item())\n",
    "print(\"Unnormalized MSE:\", unnormalized_mse.item())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "source": [
    "(unnormalized_action[:5], unnormalized_action_pred[:5])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "source": [
    "# Load the model.\n",
    "model = InverseDynamicsCNN(C, H, W, action_dim).to(device)\n",
    "model.load_state_dict(torch.load(\"inverse_dynamics_cnn_final.pt\"))"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
