{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "source": [
    "import collections\n",
    "import cv2\n",
    "import functools\n",
    "import imageio\n",
    "import os\n",
    "import sys\n",
    "from typing import Callable, Optional\n",
    "\n",
    "if \"PyTorch_VAE\" not in sys.path:\n",
    "    sys.path.append(\"PyTorch_VAE\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "from diffusion_policy.dataset.pusht_image_dataset import PushTImageDataset\n",
    "from diffusion_policy.model.diffusion import conditional_unet1d\n",
    "from diffusion_policy.env.pusht.pusht_image_env import PushTImageEnv\n",
    "from inverse_dynamics import InverseDynamicsCNN\n",
    "from state_diffusion import Diffusion, sample\n",
    "from utils import (\n",
    "    normalize_pn1,\n",
    "    normalize_standard_normal,\n",
    "    denormalize_pn1,\n",
    "    denormalize_standard_normal,\n",
    ")\n",
    "from vae import VanillaVAE"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "source": [
    "device = \"cuda\"\n",
    "data_path = (\n",
    "    \"/nas/ucb/ebronstein/lsdp/diffusion_policy/data/pusht/pusht_cchi_v7_replay.zarr\"\n",
    ")\n",
    "\n",
    "# VAE\n",
    "vae_path = \"models/pusht_vae/vae_32_20240403.pt\"\n",
    "vae_latent_dim = 32\n",
    "\n",
    "# Diffusion\n",
    "diffusion_load_dir = \"models/diffusion/pusht_unet1d_img_128_256_512_1024_edim_256_obs_8_pred_8_bs_256_lr_0.0003_e_250_ema_norm_latent_uniform/2024-05-06_01-09-27\"\n",
    "n_obs_history = 8\n",
    "n_pred_horizon = 8\n",
    "diffusion_step_embed_dim = 256\n",
    "normalize_latent = \"uniform\"\n",
    "use_ema_helper = True\n",
    "lr = 3e-4\n",
    "\n",
    "# Inverse dynamics\n",
    "inv_dyn_path = \"models/inverse_dynamics/pusht_cnn-img-obs_5-bs_256-lr_0.0001-epochs_10-train_on_recon-False-latent_dim_32/2024-05-06_17-04-08/inverse_dynamics_final.pt\"\n",
    "inv_dyn_n_obs_history = 5"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "dataset = PushTImageDataset(data_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "source": [
    "# Load the VAE\n",
    "img_data = (\n",
    "    torch.from_numpy(dataset.replay_buffer[\"img\"]).permute(0, 3, 1, 2).float()\n",
    ")\n",
    "N, C, H, W = img_data.shape\n",
    "vae = VanillaVAE(in_channels=C, in_height=H, in_width=W, latent_dim=vae_latent_dim).to(\n",
    "    device\n",
    ")\n",
    "vae.load_state_dict(torch.load(vae_path))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "# Load the diffusion model.\n",
    "if n_pred_horizon == 1:\n",
    "    down_dims = [128, 256]\n",
    "elif n_pred_horizon == 4:\n",
    "    down_dims = [128, 256, 512]\n",
    "elif n_pred_horizon == 8:\n",
    "    down_dims = [128, 256, 512, 1024]\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "STATE_DIM = vae_latent_dim\n",
    "global_cond_dim = STATE_DIM * n_obs_history\n",
    "diff_model = conditional_unet1d.ConditionalUnet1D(\n",
    "    input_dim=STATE_DIM,\n",
    "    down_dims=down_dims,\n",
    "    diffusion_step_embed_dim=diffusion_step_embed_dim,\n",
    "    global_cond_dim=global_cond_dim,\n",
    ").to(device)\n",
    "\n",
    "# Make the observation normalizer.\n",
    "if normalize_latent == \"uniform\":\n",
    "    latent_min = np.load(os.path.join(diffusion_load_dir, \"latent_min.npy\"))\n",
    "    latent_max = np.load(os.path.join(diffusion_load_dir, \"latent_max.npy\"))\n",
    "    obs_normalizer = functools.partial(\n",
    "        normalize_pn1,\n",
    "        min_val=torch.tensor(latent_min, dtype=torch.float32, device=device),\n",
    "        max_val=torch.tensor(latent_max, dtype=torch.float32, device=device),\n",
    "    )\n",
    "elif normalize_latent == \"standard_normal\":\n",
    "    latent_mean = np.load(os.path.join(diffusion_load_dir, \"latent_mean.npy\"))\n",
    "    latent_std = np.load(os.path.join(diffusion_load_dir, \"latent_std.npy\"))\n",
    "    obs_normalizer = functools.partial(\n",
    "        normalize_standard_normal,\n",
    "        mean=torch.tensor(latent_mean, dtype=torch.float32, device=device),\n",
    "        std=torch.tensor(latent_std, dtype=torch.float32, device=device),\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError()\n",
    "\n",
    "optim_kwargs = dict(lr=lr)\n",
    "diffusion = Diffusion(\n",
    "    train_data=None,\n",
    "    test_data=None,\n",
    "    obs_normalizer=obs_normalizer,\n",
    "    model=diff_model,\n",
    "    n_epochs=0,\n",
    "    optim_kwargs=optim_kwargs,\n",
    "    device=device,\n",
    "    use_ema_helper=use_ema_helper,\n",
    ")\n",
    "diffusion.load(os.path.join(diffusion_load_dir, \"diffusion_model_final.pt\"))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "source": [
    "# Load the inverse dynamics model.\n",
    "\n",
    "hidden_dims = None\n",
    "N, H, W, C = dataset.replay_buffer[\"img\"].shape\n",
    "N, action_dim = dataset.replay_buffer[\"action\"].shape\n",
    "inv_dyn_model = InverseDynamicsCNN(\n",
    "    C, H, W, action_dim, inv_dyn_n_obs_history, hidden_dims=hidden_dims\n",
    ").to(device)\n",
    "\n",
    "inv_dyn_model.load_state_dict(torch.load(inv_dyn_path))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "source": [
    "# Number of actions predicted.\n",
    "pred_horizon = 8\n",
    "# Number of observations in the history.\n",
    "obs_horizon = 8\n",
    "# Number of actions executed open-loop.\n",
    "action_horizon = 1\n",
    "action_dim = 2\n",
    "\n",
    "# |o|o|                             observations: 2\n",
    "# | |a|a|a|a|a|a|a|a|               actions executed: 8\n",
    "# |p|p|p|p|p|p|p|p|p|p|p|p|p|p|p|p| actions predicted: 16\n",
    "\n",
    "# 0. create env object\n",
    "env = PushTImageEnv()\n",
    "\n",
    "# 1. seed env for initial state.\n",
    "# Seed 0-200 are used for the demonstration dataset.\n",
    "seed = 0\n",
    "env.seed(seed)\n",
    "\n",
    "# 2. must reset before use\n",
    "obs = env.reset()\n",
    "\n",
    "# 3. 2D positional action space [0,512]\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# 4. Standard gym step method\n",
    "obs, reward, terminated, truncated = env.step(action)\n",
    "\n",
    "# prints and explains each dimension of the observation and action vectors\n",
    "with np.printoptions(precision=4, suppress=True, threshold=5):\n",
    "    print(\"obs['image'].shape:\", obs[\"image\"].shape, \"float32, [0,1]\")\n",
    "    print(\"obs['agent_pos'].shape:\", obs[\"agent_pos\"].shape, \"float32, [0,512]\")\n",
    "    print(\"action.shape: \", action.shape, \"float32, [0,512]\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "source": [
    "# Action normalization functions.\n",
    "denormalize_action = functools.partial(\n",
    "    denormalize_pn1,\n",
    "    min_val=torch.tensor(env.action_space.low, dtype=torch.float32, device=device),\n",
    "    max_val=torch.tensor(env.action_space.high, dtype=torch.float32, device=device),\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "source": [
    "assert action_horizon == 1, \"Only action_horizon=1 is supported.\"\n",
    "\n",
    "# limit environment interaction to 200 steps before termination\n",
    "max_steps = 50\n",
    "# Number of diffusion samples per step for rejection sampling.\n",
    "num_samples_per_step = 64\n",
    "\n",
    "# keep a queue of last obs_horizon steps of observations\n",
    "obs_deque = collections.deque([obs] * obs_horizon, maxlen=obs_horizon)\n",
    "# save visualization and rewards\n",
    "imgs = [env.render(mode=\"rgb_array\")]\n",
    "rewards = list()\n",
    "done = False\n",
    "step_idx = 0\n",
    "\n",
    "npred_list = []\n",
    "images_list = []\n",
    "\n",
    "with tqdm(total=max_steps, desc=\"Eval PushTImageEnv\") as pbar:\n",
    "    while not done:\n",
    "        B = 1\n",
    "        # stack the last obs_horizon number of observations\n",
    "        # Range: [0, 1]\n",
    "        images = np.stack([x[\"image\"] for x in obs_deque])  # [obs_horizon, 3, 96, 96]\n",
    "        images_list.append(images)\n",
    "        # Range: [0, 512]\n",
    "        agent_poses = np.stack([x[\"agent_pos\"] for x in obs_deque])  # [obs_horizon, 2]\n",
    "\n",
    "        # normalize observation\n",
    "        # double check this\n",
    "        nagent_poses = agent_poses / 512.0 * 2 - 1  # [-1, 1]\n",
    "        assert nagent_poses.min() >= -1 and nagent_poses.max() <= 1\n",
    "\n",
    "        # images are normalized to [0,1], in the environment\n",
    "        nimages = images * 2 - 1  # [-1, 1.]\n",
    "        assert nimages.min() >= -1 and nimages.max() <= 1\n",
    "\n",
    "        # device transfer\n",
    "        nimages = torch.from_numpy(nimages).to(device, dtype=torch.float32)\n",
    "        # (2,3,96,96)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Encode images with the VAE.\n",
    "            obs_latents = vae.encode(nimages)[0]\n",
    "            obs_latents = obs_latents.repeat(num_samples_per_step, 1, 1)\n",
    "            # print(\"obs_latents:\", obs_latents.shape)\n",
    "            # print(\"obs_latents.min():\", obs_latents.min())\n",
    "            # print(\"obs_latents.max():\", obs_latents.max())\n",
    "\n",
    "            # No need to normalize the VAE latents here because we do so in\n",
    "            # the sample functions.\n",
    "            # # Range: [-1, 1]\n",
    "            # nobs_latents = obs_normalizer(obs_latents)\n",
    "            # assert nobs_latents.min() >= -1 and nobs_latents.max() <= 1\n",
    "\n",
    "            # Diffuse the latent states.\n",
    "            rejection_sample_count = 0\n",
    "            while True:\n",
    "                # [len(return_steps), num_samples, n_pred_horizon, latent_dim] and\n",
    "                # [len(return_steps), num_samples, n_obs_history, latent_dim)]\n",
    "                npred_latents, _ = sample(\n",
    "                    diffusion,\n",
    "                    num_samples=num_samples_per_step,\n",
    "                    return_steps=[512],\n",
    "                    data_shape=(n_pred_horizon, STATE_DIM),\n",
    "                    obs_data=obs_latents,\n",
    "                    obs_normalizer=obs_normalizer,\n",
    "                    clip=None,\n",
    "                    clip_noise=(-3, 3),\n",
    "                    device=device,\n",
    "                    obs_key=None,  # Unused because obs_data is not a dataloader.\n",
    "                )\n",
    "                # print(\"npred_latents:\", npred_latents.shape)\n",
    "                # print(\"npred_latents.min()\", npred_latents.min())\n",
    "                # print(\"npred_latents.max()\", npred_latents.max())\n",
    "                # [num_samples, n_pred_horizon, latent_dim]\n",
    "                npred_latents = npred_latents.squeeze(0)\n",
    "                # print(\"npred_latents:\", npred_latents.shape)\n",
    "\n",
    "                if normalize_latent == \"uniform\":\n",
    "                    # Remove samples that are out of the range [-1, 1].\n",
    "                    mask = ((npred_latents >= -1) & (npred_latents <= 1)).all(\n",
    "                        axis=(-1, -2)\n",
    "                    )\n",
    "                    # Range: [-1, 1] (for real now)\n",
    "                    if mask.sum() > 0:\n",
    "                        # [n_in_range_samples, n_pred_horizon, latent_dim]\n",
    "                        in_range_npred_latents = npred_latents[mask]\n",
    "                        # print(\"in_range_npred_latents:\", in_range_npred_latents.shape)\n",
    "                        break\n",
    "                    print(f\"Attempt {rejection_sample_count + 1}: mo samples in range.\")\n",
    "                else:\n",
    "                    in_range_npred_latents = npred_latents\n",
    "                    break\n",
    "\n",
    "                rejection_sample_count += 1\n",
    "\n",
    "            # Denormalize using the latents statistics.\n",
    "            if normalize_latent == \"standard_normal\":\n",
    "                # Range: [-1, 1]\n",
    "                in_range_pred_latents = denormalize_standard_normal(\n",
    "                    in_range_npred_latents, latent_mean, latent_std\n",
    "                )\n",
    "            elif normalize_latent == \"uniform\":\n",
    "                # Range: [latent_min, latent_max]\n",
    "                in_range_pred_latents = denormalize_pn1(\n",
    "                    in_range_npred_latents, latent_min, latent_max\n",
    "                )\n",
    "            else:\n",
    "                in_range_pred_latents = in_range_npred_latents\n",
    "\n",
    "            # Decode using the VAE.\n",
    "            # [n_in_range_samples * n_pred_horizon, C, H, W]\n",
    "            # Range: [-1, 1]\n",
    "            npred = vae.decode(torch.from_numpy(in_range_pred_latents).to(device))\n",
    "            # [n_in_range_samples, n_pred_horizon, C, H, W]\n",
    "            npred = npred.reshape(\n",
    "                [-1, n_pred_horizon] + list(npred.shape[1:])\n",
    "            )\n",
    "            npred_list.append(npred)\n",
    "            # print(\"npred:\", npred.shape)\n",
    "\n",
    "            # Take the first sample prediction arbitrarily.\n",
    "            npred = npred[0]  # [n_pred_horizon, C, H, W]\n",
    "            # print(\"npred:\", npred.shape)\n",
    "\n",
    "            # Concatenate the last inv_dyn_n_obs_history - 1 observations with\n",
    "            # the first predicted observation in order to get the action.\n",
    "            # NOTE: this currently only works for action_horizon = 1.\n",
    "            nobs_and_npred = torch.cat([nimages[-(inv_dyn_n_obs_history - 1):], npred[:1]], dim=0)\n",
    "\n",
    "            # NOTE: no need to denormalize the image because the inverse dynamics\n",
    "            # model takes in a normalized image.\n",
    "\n",
    "            # Run the inverse dynamics model.\n",
    "            # [1, action_dim]\n",
    "            nactions = inv_dyn_model(nobs_and_npred.unsqueeze(0))\n",
    "            # print(\"nactions:\", nactions.shape)\n",
    "            # Denormalize the actions.\n",
    "            # [1, action_dim]\n",
    "            actions = denormalize_action(nactions)\n",
    "            # print(\"actions:\", actions.shape)\n",
    "\n",
    "        # nagent_poses = torch.from_numpy(nagent_poses).to(device, dtype=torch.float32)\n",
    "        nagent_poses = nagent_poses\n",
    "        # (2,2)\n",
    "\n",
    "        # TODO: only take action_horizon number of actions\n",
    "        # action = actions[:action_horizon, :]  # (action_horizon, action_dim)\n",
    "        action = actions.cpu()  # (action_horizon = 1, action_dim)\n",
    "\n",
    "        # execute action_horizon number of steps\n",
    "        # without replanning\n",
    "        for i in range(len(action)):\n",
    "            # stepping env\n",
    "            obs, reward, done, _ = env.step(action[i])\n",
    "            # save observations\n",
    "            obs_deque.append(obs)\n",
    "            # and reward/vis\n",
    "            rewards.append(reward)\n",
    "            imgs.append(env.render(mode=\"rgb_array\"))\n",
    "\n",
    "            # update progress bar\n",
    "            step_idx += 1\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix(reward=reward)\n",
    "            if step_idx > max_steps:\n",
    "                done = True\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "# print out the maximum target coverage\n",
    "print(\"Score: \", max(rewards))\n",
    "\n",
    "# visualize\n",
    "# from IPython.display import Video\n",
    "# vwrite('vis.mp4', imgs)\n",
    "# Video('vis.mp4', embed=True, width=256, height=256)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "source": [
    "plt.imshow(obs[\"image\"].transpose(1, 2, 0))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "source": [
    "def create_gif(image_arrays, filename, duration=0.1):\n",
    "    \"\"\"\n",
    "    Create a GIF from a list of NumPy array images.\n",
    "\n",
    "    Parameters:\n",
    "        image_arrays (list of np.ndarray): List of NumPy array images.\n",
    "        filename (str): Filename for the output GIF.\n",
    "        duration (float): Duration (in seconds) of each frame in the GIF.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for image_array in image_arrays:\n",
    "        # Ensure that the image array is in uint8 format\n",
    "        image_array = np.uint8(image_array)\n",
    "        images.append(image_array)\n",
    "\n",
    "    # Write the images to a GIF file\n",
    "    with imageio.get_writer(filename, mode='I', duration=duration) as writer:\n",
    "        for image in images:\n",
    "            writer.append_data(image)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "source": [
    "create_gif(imgs, f\"pusht_rollout_{seed}.gif\", duration=0.5)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "source": [
    "# Visualize\n",
    "\n",
    "# image_folder = 'images'\n",
    "video_name = 'video.avi'\n",
    "\n",
    "# images = [img for img in os.listdir(image_folder) if img.endswith(\".png\")]\n",
    "# frame = cv2.imread(os.path.join(image_folder, images[0]))\n",
    "height, width, layers = imgs[0].shape\n",
    "\n",
    "video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'MJPG'), 0, 1, (width,height))\n",
    "\n",
    "for image in imgs:\n",
    "    video.write(image)\n",
    "\n",
    "# cv2.destroyAllWindows()\n",
    "video.release()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "source": [
    "# fig, axs = plt.subplots(1, len(imgs), figsize=(16, 4 * len(imgs)))\n",
    "for i, img in enumerate(imgs):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.imshow(img)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsdp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
