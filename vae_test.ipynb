{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-30T18:53:18.498983Z",
     "start_time": "2024-04-30T18:53:17.930762Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import functools\n",
    "import math\n",
    "import time\n",
    "from typing import Optional\n",
    "\n",
    "if \"PyTorch_VAE\" not in sys.path:\n",
    "    sys.path.append(\"PyTorch_VAE\")\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "\n",
    "from PyTorch_VAE import models\n",
    "from diffusion_policy.common.pytorch_util import compute_conv_output_shape\n",
    "from diffusion_policy.dataset.pusht_image_dataset import PushTImageDataset\n",
    "\n",
    "\n",
    "from lsdp_utils.EpisodeDataset import EpisodeDataset, EpisodeDataloaders\n",
    "from lsdp_utils.VanillaVAE import VanillaVAE\n",
    "from lsdp_utils.utils import bcolors\n",
    "from lsdp_utils.utils import plot_losses, plot_samples, normalize_pn1, denormalize_pn1, bcolors\n",
    "\n",
    "from types import SimpleNamespace\n",
    "\n",
    "cfg = SimpleNamespace(dataset_path='/home/matteogu/ssd_data/data_diffusion/pusht/pusht_cchi_v7_replay.zarr',\n",
    "                      # vae_model_path='/home/matteogu/Desktop/prj_deepul/repo_online/lsdp/models/pusht_vae/vae_32_20240403.pt',\n",
    "                      vae_save_dir='/home/matteogu/ssd_data/diffusion_models/models/vae/',\n",
    "                      n_obs_history=0, \n",
    "                      n_pred_horizon=1,  # just trying to fit the latents to states\n",
    "                      \n",
    "                      kld_weight=1e-7,\n",
    "                      latent_dim=128,\n",
    "                      hidden_dims=[32, 64, 128, 256, 512],\n",
    "\n",
    "                      train_split=0.8,\n",
    "\n",
    "                      batch_size=512,\n",
    "                      lr=5e-4,  # optimization params\n",
    "\n",
    "                      epochs=100,\n",
    "                      device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                      )\n",
    "\n",
    "\n",
    "# path = \"/nas/ucb/ebronstein/lsdp/diffusion_policy/data/pusht/pusht_cchi_v7_replay.zarr\"\n",
    "# # path = \"/home/tsadja/data_diffusion/pusht/pusht_cchi_v7_replay.zarr\"\n",
    "# path = '/home/matteogu/ssd_data/data_diffusion/pusht/pusht_cchi_v7_replay.zarr'\n",
    "\n",
    "dataset = PushTImageDataset(cfg.dataset_path)\n",
    "full_dataset = torch.from_numpy(dataset.replay_buffer[\"img\"]).permute(0, 3, 1, 2)\n",
    "\n",
    "str_hidden = str(cfg.hidden_dims)[1:-1].replace(\", \", \"_\")\n",
    "name = (f'pusht_vae_klw_{cfg.kld_weight:.2e}_ldim_{cfg.latent_dim}_'\n",
    "        f'bs_{cfg.batch_size}_epochs_{cfg.epochs}_lr_{cfg.lr}_hdim_{str_hidden}')\n",
    "save_dir = f'{cfg.vae_save_dir}{name}'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "print(f\"{bcolors.OKGREEN} ---------------------- {bcolors.ENDC}\")\n",
    "print(f\"{bcolors.OKGREEN}   {name}   {bcolors.ENDC}\")\n",
    "print(f\"{bcolors.OKGREEN} ---------------------- {bcolors.ENDC}\")\n",
    "\n"
   ],
   "execution_count": 91,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T18:53:23.495577Z",
     "start_time": "2024-04-30T18:53:19.926639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = PushTImageDataset(cfg.dataset_path)\n",
    "full_dataset = torch.from_numpy(dataset.replay_buffer[\"img\"]).permute(0, 3, 1, 2)\n",
    "N, C, H, W = full_dataset.shape\n",
    "\n",
    "# Make the state normalizer.\n",
    "max_state = dataset.replay_buffer[\"state\"].max(axis=0)\n",
    "min_state = dataset.replay_buffer[\"state\"].min(axis=0)\n",
    "\n",
    "vae_model = VanillaVAE(\n",
    "    in_channels=C, in_height=H, in_width=W, latent_dim=cfg.latent_dim\n",
    ").to(cfg.device)\n",
    "vae_model.load_state_dict(torch.load(save_dir+'/vae_99.pt'))\n",
    "\n",
    "def get_latent(x, _vae_model, device):\n",
    "    x = x / 255.0\n",
    "    x = 2. * x - 1.\n",
    "    return _vae_model.encode(torch.from_numpy(x).to(device))[0].detach()\n",
    "\n",
    "normalize_encoder_input = functools.partial(\n",
    "    get_latent, _vae_model=vae_model, device=cfg.device\n",
    ")\n",
    "\n",
    "\n",
    "state_normalizer = functools.partial(\n",
    "    normalize_pn1, min_val=min_state, max_val=max_state\n",
    ")\n",
    "\n",
    "process_fns = {\"state\": state_normalizer, \"img\": normalize_encoder_input}\n",
    "\n",
    "print(\"Making datasets and dataloaders.\")\n",
    "train_loader, val_loader = EpisodeDataloaders(dataset=dataset,\n",
    "                                              include_keys=[\"state\", \"img\"],  # one key only\n",
    "                                              process_fns=process_fns,\n",
    "                                              cfg=cfg,\n",
    "                                              val_ratio=0.9)  # configuration params\n"
   ],
   "id": "c5260adf72064027",
   "execution_count": 92,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TRAIN MAPPING LATENTS TO STATE. need to normalize, encoder output",
   "id": "1f7c65bfb51b011e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T18:53:29.109325Z",
     "start_time": "2024-04-30T18:53:28.914971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_latents = len(val_loader.dataset.samples)\n",
    "# with torch.no_grad():\n",
    "#     for i, sample in enumerate(val_loader):\n",
    "#         input, target = sample[1]['img'], sample[1]['state']\n",
    "\n",
    "all_latents = torch.zeros(n_latents, cfg.latent_dim)\n",
    "for idx_sample in trange(n_latents):\n",
    "    all_latents[idx_sample] = val_loader.dataset.samples[idx_sample][1]['img']  # latent\n",
    "# val_loader.dataset.samples[idx_sample][1]['img']  # latent\n",
    "\n",
    "min_latents = torch.min(all_latents, axis=0).values.to(cfg.device)\n",
    "max_latents = torch.max(all_latents, axis=0).values.to(cfg.device)\n",
    "\n",
    "#between 0 and 1\n",
    "norm_latents =  (all_latents - min_latents.cpu())/(max_latents.cpu() - min_latents.cpu())\n",
    "\n",
    "norm_latents.min(), norm_latents.max(), "
   ],
   "id": "4ac1a6af8b5217d5",
   "execution_count": 95,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T18:24:13.618910Z",
     "start_time": "2024-04-30T18:24:13.614323Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample = next(iter(train_loader))\n",
    "sample[1]['img'].shape, sample[1]['state'].shape\n"
   ],
   "id": "6b2af27441767a77",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T18:53:33.760804Z",
     "start_time": "2024-04-30T18:53:33.757621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LatentsToStateMLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_dim, out_dim, hidden_dims: list[int]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, out_dim))\n",
    "        # layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x = obs_history.flatten(start_dim=1)\n",
    "        x = self.model(x)\n",
    "        return x"
   ],
   "id": "8142187521a5ec29",
   "execution_count": 96,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T19:02:29.638709Z",
     "start_time": "2024-04-30T19:02:29.630976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = LatentsToStateMLP(in_dim=cfg.latent_dim, \n",
    "                          out_dim=5,  # state\n",
    "                          hidden_dims=[256, 256, 128, 16]).to(cfg.device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n"
   ],
   "id": "a4f81f19bec04d29",
   "execution_count": 100,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T19:02:51.473194Z",
     "start_time": "2024-04-30T19:02:31.525848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 400\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "train_start_time = time.time()\n",
    "for epoch in trange(EPOCHS):\n",
    "    total_train_loss = 0\n",
    "    model.train()\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        input, target = sample[1]['img'], sample[1]['state']\n",
    "        \n",
    "        norm_input = input #(input - min_latents)/(max_latents - min_latents)\n",
    "        \n",
    "        result = model(norm_input)\n",
    "        loss = criterion(result, target)\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # print(f\"Train loss: {total_train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    total_test_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, sample in enumerate(val_loader):\n",
    "            input, target = sample[1]['img'], sample[1]['state']\n",
    "\n",
    "            norm_input = input # (input - min_latents)/(max_latents - min_latents)\n",
    "\n",
    "            result = model(norm_input)\n",
    "            loss = criterion(result, target)\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "    test_losses.append(total_test_loss / len(val_loader))\n",
    "    if epoch % 30 == 0:\n",
    "        print(f\"[Val-{epoch}] loss: {test_losses[-1]:.2e}\")\n",
    "print(f'Saving.. {datetime.timedelta(seconds=time.time() - train_start_time)}')\n"
   ],
   "id": "575e2a822399b252",
   "execution_count": 101,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T19:02:56.926461Z",
     "start_time": "2024-04-30T19:02:56.833488Z"
    }
   },
   "cell_type": "code",
   "source": "plot_losses(train_losses, test_losses)",
   "id": "6896a26454aa4971",
   "execution_count": 102,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T19:13:13.160586Z",
     "start_time": "2024-04-30T19:13:13.157003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n"
   ],
   "id": "91edfaf3bc975392",
   "execution_count": 112,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T19:03:57.830568Z",
     "start_time": "2024-04-30T19:03:57.823287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.save(model.state_dict(), \n",
    "           os.path.join(f\"/home/matteogu/ssd_data/diffusion_models/models\"\n",
    "                        f\"/latent_to_state/mlp_128to5.pt\"))"
   ],
   "id": "a96c2e6549939264",
   "execution_count": 103,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sqllm",
   "language": "python",
   "name": "sqllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
